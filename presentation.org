#+TITLE: High-Performance Matrix Multiplication in Rust
#+AUTHOR: Olav
#+DATE: 2025-10-17
#+STARTUP: overview
#+OPTIONS: toc:nil num:nil

* Introduction

High-Performance Matrix Multiplication in Rust

* Why Matrix Multiplication Matters
Matrix multiplication is *everywhere* in modern computing:
- *Your screen right now*: Graphics transforms
- *DOM rendering*: 2D transform matrices
- *Video playback*: FFT, compression/decompression
- *AI/ML inference*: neural network layers
- *Audio processing*: FFT, digital signal processing, equalizers, spatial audio

*The bottleneck*: CPUs perform ~10‚Åπ operations/second, but naive matrix multiplication is slow

We can do better.
* What is Matrix Multiplication?
** Scalars
5 * 3 = 15

* What is Matrix Multiplication?
** Scalars
** Vectors

         
         
                  
[ 5 3 1 ]
         
* What is Matrix Multiplication?
** Scalars
** Vectors

         [ 2
           6
           1 ]        
[ 5 3 1 ] = [ 5 * 2
            + 3 * 6
            + 1 * 1 ]
          = 29
* What is Matrix Multiplication?
** Scalars
** Vectors

         [ 2
           6
           1 ]        
[ 5 3 1 ] = [ 5 * 2
            + 3 * 6
            + 1 * 1 ]
          = 29
          
Also called the dot product.

* What is Matrix Multiplication?
** Scalars
** Vectors
** Matrices
manim!

* What is Matrix Multiplication?
** Lots of mul and add
** Memory Access
** Highly parallelizable
** Algorithmic complexity

* What is Matrix Multiplication?
** Lots of mul and add
** Memory Access
Read heavy!
** Highly parallelizable 
Each resulting C[i][j] can be computed separately!
** Algorithmic complexity
O(n^3)
O(n^2.8)

* Starting Point: Naive data struct

#+BEGIN_SRC rust
pub struct RowMajorMatrix {
    pub rows: usize,
    pub cols: usize,
    pub data: Vec<f64>,
}
#+END_SRC
* Starting Point: Naive Implementation

#+BEGIN_SRC rust
// Square n*n matrices
for i in 0..n {
    for j in 0..n {
        for k in 0..n {
            c[i][j] += a[i][k] * b[k][j];
        }
    }
}
#+END_SRC

*The problem*: `b[k][j]` causes cache misses - jumping across rows in memory!
* Constraints
** Compute Constraints
- Multiple ALUs (compute units) per core
  - Can do ~2-4 operations per cycle (mul + add)
- Instruction Pipeline
  - Pipeline stalls on dependencies, branches, cache misses

* Constraints
** Memory Constraints
- Fast CPU
- Slow memory
  
- Memory Layout
  - L1d cache (~4 cycles latency)
  - L2 cache (~12 cycles latency)
  - L3 cache (~40 cycles latency)
  - RAM ~100-300 cycles latency)
* Constraints
** Memory layout
 [[ 1 2 ]
  [ 3 4] ]

* Constraints
** Memory layout
 [[ 1 2 ]
  [ 3 4] ]

Row major
[1 2 3 4]

* Constraints
** Memory layout
 [[ 1 2 ]
  [ 3 4] ]

Row major
[1 2 3 4]

Column major
[1 3 2 4]

* Constraints
** Cache lines
Load 64 bytes at a time (8√ó f64)
** Maybe more manim(!!)
* Constraints
** Our goal
*** Maximize data reuse
*** Minimize memory traffic
*** Keep ALUs fed
* The Journey from Naive to Optimized

Four key optimization strategies:

1. Column-major layout - Fix cache locality
2. Cache blocking (tiling) - Fit data in L1 cache
3. SIMD vectorization - Process 4 numbers at once
4. Performance profiling - Find and fix hidden bottlenecks

* Live Demo: Naive Performance

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul

cargo +nightly bench --bench naive_128

#+END_SRC

#+RESULTS:
: naive_128               time:   [2.3472 ms 2.3670 ms 2.3909 ms]
:                         change: [+1.2378% +2.1371% +3.1845%] (p = 0.00 < 0.05)
:                         Performance has regressed.
: Found 11 outliers among 100 measurements (11.00%)
:   4 (4.00%) high mild
:   7 (7.00%) high severe
: 

* Strategy 1: Column-Major Layout

** Problem:
Row-major matrix B accessed by columns ‚Üí cache disaster
** Solution:
Store matrix in column-major order for efficient column access

* Strategy 1: Column-Major Layout

** Problem:
Row-major matrix B accessed by columns ‚Üí cache disaster
** Solution:
Store matrix in column-major order for efficient column access

#+BEGIN_SRC rust :exports code
// Now accessing B by columns is cache-friendly!
for i in 0..n {
    for j in 0..n {
        for k in 0..n {
            c[i][j] += a[i][k] * b_col_major[j][k];  // Sequential!
        }
    }
}
#+END_SRC

* Strategy 2: Cache Blocking (Tiling)

Cache blocking - the 6-nested loop algorithm:
- 3 outer loops iterate over blocks
- 3 inner loops compute within blocks
- Split matrices into 64√ó64 blocks (32KB fits in L1d: 37KB)
- Process blocks instead of full rows/columns
- Reuse data while it's still in cache

* Block Size Analysis

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release --bin matmul -- --blocked
#+END_SRC

#+RESULTS:
#+begin_example
Blocked Matrix Multiplication Comparison
size,algorithm,block_size,time_ns,gflops
256,naive,0,24233658,1.384621
256,optimized,0,13479549,2.489284
256,blocked,32,18253015,1.838295
256,blocked,48,17011042,1.972509
256,blocked,64,20912923,1.604483
256,blocked,96,20753875,1.616779
256,blocked,128,19576060,1.714054
256,blocked_ultimate,64,9434899,3.556417
512,naive,0,557058558,0.481880
512,optimized,0,105669088,2.540340
512,blocked,32,147348918,1.821767
512,blocked,48,147408844,1.821027
512,blocked,64,183439190,1.463348
512,blocked,96,182888670,1.467753
512,blocked,128,174111424,1.541745
512,blocked_ultimate,64,74898344,3.583997
768,naive,0,1530656653,0.591883
768,optimized,0,357349543,2.535248
768,blocked,32,479525662,1.889304
768,blocked,48,512926613,1.766275
768,blocked,64,542397196,1.670307
768,blocked,96,613639509,1.476387
768,blocked,128,583112422,1.553679
768,blocked_ultimate,64,248575943,3.644639
1024,naive,0,4736583120,0.453382
1024,optimized,0,854573451,2.512930
1024,blocked,32,1191180442,1.802820
1024,blocked,48,1229022280,1.747311
1024,blocked,64,1322480954,1.623830
1024,blocked,96,1458849133,1.472040
1024,blocked,128,1381641902,1.554298
1024,blocked_ultimate,64,614751652,3.493254
#+end_example

*Key findings*:
- Block size 32-64 typically optimal for this hardware (L1d = 37KB)
- Speedup increases with matrix size
- Combined optimizations (blocked + SIMD + column-major) provide best results
- Performance varies significantly with hardware specs

* Strategy 3: SIMD Vectorization

*AVX2*: Process 4 double-precision floats in parallel

#+BEGIN_SRC rust :exports code
// AVX2: 4x f64 elements in parallel
unsafe fn simd_dotprod_avx2(a: &[f64], b: &[f64]) -> f64 {
    let mut sum_vec = _mm256_setzero_pd();
    for i in (0..len).step_by(4) {
        let a_vec = _mm256_loadu_pd(a.as_ptr().add(i));
        let b_vec = _mm256_loadu_pd(b.as_ptr().add(i));
        sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(a_vec, b_vec));
    }
    // Sum horizontal + remainder
}
#+END_SRC

*Benefit*: 4x computational throughput (when not memory-bound)

* Strategy 4: Performance Profiling with perf

Key metrics:
- L1 dcache load misses (cache locality)
- Instructions per cycle (IPC)
- Memory Bound vs Core Bound (what's limiting us?)
- Retiring percentage (useful work vs stalls)

* Strategy 4: Performance Profiling with perf

Key metrics:
- L1 dcache load misses (cache locality)
- Instructions per cycle (IPC)
- Memory Bound vs Core Bound (what's limiting us?)
- Retiring percentage (useful work vs stalls)

- perf [--version] [--help] [OPTIONS] COMMAND [ARGS]

* Hardware Performance Counters: The Insights (512√ó512)

| Metric                | Naive      | Blocked    | Optimized  | nalgebra   |
|-----------------------+------------+------------+------------+------------|
| *L1 miss rate*        | ~48%       | ~46%       | ~2-8%      | ~3%        |
| *Backend Bound*       | ~77%       | ~13%       | ~22%       | ~41%       |
| *Memory Bound*        | ~42%       | ~2%        | ~8%        | ~4%        |
| *Retiring (useful)*   | ~21%       | ~65%       | ~63%       | ~57%       |

*Key insights*:
- Naive spends ~42% of time waiting on memory - *cache catastrophe*!
- Blocked fixes L3 but not L1 - still ~46% L1 miss rate
- Optimized + SIMD: *Dramatically fewer L1 misses* (~50√ó better)
- Blocked does *3√ó more useful work* (65% retiring vs 20%)
- nalgebra executes *far fewer instructions* - algorithmic superiority

*Note*: Relative improvements are consistent; absolute values vary by hardware.

* Live Demo: Profiling Comparison

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Compare cache behavior of all implementations
./compare_perf.sh 128 cache 2>&1 | grep -E "(Profiling|Time:|L1-dcache-load-misses)" | head -20
#+END_SRC

Watch how L1 miss rate drops:
- Naive: ~48% (cache catastrophe)
- Blocked: ~46% (helps L3, not L1)
- SIMD: ~8% (column-major helps significantly)
- Optimized: ~2-5% (much better!)
  
* The Smoking Gun: 49% Overhead

Perf profiling revealed we were spending 49% of execution time on *redundant work*:

#+BEGIN_SRC rust
for i in ii..i_end {
    fill_row_buf(i);           // Load A data: 4,096 elements
    for j in jj..j_end {
        fill_col_buf(j);       // ‚Üê PROBLEM: Loaded 4,096 times!
        dotprod(row_buf, col_buf);
    }
}
#+END_SRC

*The bug*: Column buffer filled once per (i,j) pair instead of once per j
*Result*: 262,144 redundant buffer fills per 64√ó64 block

* The Fix: Reuse Column Buffers

#+BEGIN_SRC rust
// Pre-fill ALL column buffers once
let col_bufs = prefill_all_columns(jj..j_end);  // 64 buffers

for i in ii..i_end {
    fill_row_buf(i);
    for j in jj..j_end {
        let col_buf = &col_bufs[j];  // ‚Üê REUSE!
        dotprod(row_buf, col_buf);
    }
}
#+END_SRC

*Impact*:
- Performance: *~2√ó additional speedup*
- Instructions: *~2.8√ó fewer*
- L1 miss rate: *~4-5√ó better*

* What Didn't Work: FMA Instructions

Before profiling, we tried *FMA* (fused multiply-add) - single instruction for multiply+add:

#+BEGIN_SRC rust
// SIMD: separate multiply + add
let prod = _mm256_mul_pd(a_vec, b_vec);
sum_vec = _mm256_add_pd(sum_vec, prod);

// FMA: fused multiply-add (faster, right?)
sum_vec = _mm256_fmadd_pd(a_vec, b_vec, sum_vec);
#+END_SRC

*Result*: *0% improvement!*

*Why?* We weren't bottlenecked by multiply-add operations. The real problem was algorithmic (redundant buffer fills).

*Lesson*: Don't guess! Profile first, then optimize what actually matters.

* Performance Comparison: The Numbers

512√ó512 matrices:

| Implementation | L1 Miss Rate | Speedup vs Naive |
|----------------|--------------|------------------|
| Naive          | ~48% üí•      | 1.0√ó             |
| Blocked        | ~46%         | ~3-4√ó            |
| SIMD           | ~8%          | ~7-8√ó            |
| *Optimized*    | *~2-5%* ‚úÖ   | *~10-15√ó*        |
| nalgebra       | ~3%          | ~50√ó+            |

*Gap to production library*: Still several times slower, but much better!

*Note*: Exact speedups vary by hardware; relative improvements are consistent.

* Live Benchmark: Full Scaling Analysis

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release -- --scaling 1 | grep -E "(16x16|128x128|512x512|1024x1024)" | head -12
#+END_SRC

* Why GPUs Excel at Matrix Multiplication

*Massive parallelism*: While CPUs have 4-16 cores, GPUs have thousands of cores

#+BEGIN_QUOTE
"Intel Alder Lake: 12 cores, ~3 GHz
NVIDIA RTX 4090: 16,384 CUDA cores, ~2.5 GHz

Matrix multiplication is *embarrassingly parallel* - perfect for GPUs!"
#+END_QUOTE

* ArrayFire: High-Performance GPU Computing in Rust

#+BEGIN_SRC rust :exports code
use arrayfire::*;

// Create matrices on GPU
let a = randu::<f32>(dim4!(1024, 1024, 1, 1));
let b = randu::<f32>(dim4!(1024, 1024, 1, 1));

// GPU matrix multiplication (single line!)
let c = matmul(&a, &b, MatProp::NONE, MatProp::NONE);

// ArrayFire handles:
// - Memory transfer to/from GPU
// - Optimal kernel selection
// - Multi-device management
#+END_SRC

* GPU Performance Potential

Theoretical speedup for 1024√ó1024:

| Implementation | Time (est.) | Speedup vs Naive |
|----------------+-------------+------------------|
| Naive CPU      | ~2400ms     | 1x               |
| Our SIMD       | 463ms       | *5.2x*           |
| GPU (OpenCL)   | ~50-100ms   | *24-48x*         |

*Note*: GPU shines for large matrices (>1024√ó1024), smaller sizes suffer from transfer overhead

* What We Learned

1. *Profile first, optimize second* - FMA gave 0%, profiling found 49% overhead
2. *Cache hierarchy is multi-level* - Different optimizations target L1 vs L3
3. *Algorithmic > Instruction-level* - Buffer reuse (~2√ó gain) beat FMA (0% gain)
4. *Measure with hardware counters* - L1 miss rates reveal true bottlenecks
5. *Performance is relative* - Hardware differences matter; focus on relative gains

* Next Steps
** What Works
- ‚úÖ Cache blocking (3.7x improvement)
- ‚úÖ Column-major layout (reduces L1 misses 5x)
- ‚úÖ SIMD vectorization (parallelism)
- ‚úÖ Profiling-driven optimization (found 49% overhead)

** What Doesn't
- ‚ùå FMA instructions (0% improvement when algorithm bottlenecked)
- ‚ùå More blocking alone (can't fix buffer copy overhead)

** To Match Production Libraries
- Register tiling (keep 4√ó4 sub-blocks in registers)
- Micro-kernels (eliminate intermediate buffers)
- Matrix packing (optimize memory layout once)
- Multi-threading (Rayon for outer loops)
