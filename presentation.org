#+TITLE: High-Performance Matrix Multiplication in Rust
#+AUTHOR: Olav
#+DATE: 2025-10-17
#+STARTUP: overview
#+OPTIONS: toc:nil num:nil

* Introduction

High-Performance Matrix Multiplication in Rust

* Why Matrix Multiplication Matters
Matrix multiplication is *everywhere* in modern computing:
- *Your screen right now*: Graphics transforms
- *DOM rendering*: 2D transform matrices
- *Video playback*: FT, compression/decompression
- *AI/ML inference*: neural network layers
- *Audio processing*: FT, digital signal processing, equalizers, spatial audio

*The bottleneck*: CPUs perform ~10‚Åπ operations/second, but naive matmul is naive

We can do better.
* What is Matrix Multiplication?
** Scalars
5 * 3 = 15

* What is Matrix Multiplication?
** Scalars
** Vectors

         
         
                  
[ 5 3 1 ]
         
* What is Matrix Multiplication?
** Scalars
** Vectors

         [ 2
           6
           1 ]        
[ 5 3 1 ] = [ 5 * 2
            + 3 * 6
            + 1 * 1 ]
          = 29
* What is Matrix Multiplication?
** Scalars
** Vectors

         [ 2
           6
           1 ]        
[ 5 3 1 ] = [ 5 * 2
            + 3 * 6
            + 1 * 1 ]
          = 29
          
Also called the dot product.

* What is Matrix Multiplication?
** Scalars
** Vectors
** Matrices
manim!

* What is Matrix Multiplication?
** Lots of mul and add
** Memory Access
** Highly parallelizable
** Algorithmic complexity

* What is Matrix Multiplication?
** Lots of mul and add
** Memory Access
Read heavy!
** Highly parallelizable 
Each resulting C[i][j] can be computed separately!
** Algorithmic complexity
O(n^3)

* Starting Point: Naive Implementation

#+BEGIN_SRC rust
// Square n*n matrices
for i in 0..n {
    for j in 0..n {
        for k in 0..n {
            c[i][j] += a[i][k] * b[k][j];
        }
    }
}
#+END_SRC

*The problem*: `b[k][j]` causes cache misses - jumping across rows in memory!
* Constraints
** Compute Constraints
- Multiple ALUs (compute units) per core
  - Can do ~2-4 operations per cycle (mul + add)
- Instruction Pipeline
  - Pipeline stalls on dependencies, branches, cache misses

* Constraints
** Memory Constraints
- Fast CPU
- Slow memory
  
- Memory Layout
  - L1d cache (~4 cycles latency)
  - L2 cache (~12 cycles latency)
  - L3 cache (~40 cycles latency)
  - RAM ~100-300 cycles latency)
* Constraints
** Memory layout
 [[ 1 2 ]
  [ 3 4] ]

* Constraints
** Memory layout
 [[ 1 2 ]
  [ 3 4] ]

Row major
[1 2 3 4]

* Constraints
** Memory layout
 [[ 1 2 ]
  [ 3 4] ]

Row major
[1 2 3 4]

Column major
[1 3 2 4]

* Constraints
** Cache lines
Load 64 bytes at a time (8√ó f64)
** Maybe more manim(!!)
* Constraints
** Our goal
*** Maximize data reuse
*** Minimize memory traffic
*** Keep ALUs fed
* The Journey from Naive to Optimized

Four key optimization strategies:

1. Column-major layout - Fix cache locality
2. Cache blocking (tiling) - Fit data in L1 cache
3. SIMD vectorization - Process 4 numbers at once
4. Performance profiling - Find and fix hidden bottlenecks

* Live Demo: Naive Performance

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul

cargo +nightly bench --bench naive_128

#+END_SRC

#+RESULTS:
: naive_128               time:   [2.3137 ms 2.3175 ms 2.3213 ms]
:                         change: [+1.4671% +1.6416% +1.8169%] (p = 0.00 < 0.05)
:                         Performance has regressed.
: 

* Strategy 1: Column-Major Layout

** Problem:
Row-major matrix B accessed by columns ‚Üí cache disaster
** Solution:
Store matrix in column-major order for efficient column access

* Strategy 1: Column-Major Layout

** Problem:
Row-major matrix B accessed by columns ‚Üí cache disaster
** Solution:
Store matrix in column-major order for efficient column access

#+BEGIN_SRC rust :exports code
// Now accessing B by columns is cache-friendly!
for i in 0..n {
    for j in 0..n {
        for k in 0..n {
            c[i][j] += a[i][k] * b_col_major[j][k];  // Sequential!
        }
    }
}
#+END_SRC

* Strategy 2: Cache Blocking (Tiling)

Cache blocking - the 6-nested loop algorithm:
- 3 outer loops iterate over blocks
- 3 inner loops compute within blocks
- Split matrices into 64√ó64 blocks (32KB fits in L1d: 37KB)
- Process blocks instead of full rows/columns
- Reuse data while it's still in cache

* Block Size Analysis

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release --bin matmul -- --blocked
#+END_SRC

#+RESULTS:
#+begin_example
Blocked Matrix Multiplication Comparison
size,algorithm,block_size,time_ns,gflops
256,naive,0,24233658,1.384621
256,optimized,0,13479549,2.489284
256,blocked,32,18253015,1.838295
256,blocked,48,17011042,1.972509
256,blocked,64,20912923,1.604483
256,blocked,96,20753875,1.616779
256,blocked,128,19576060,1.714054
256,blocked_ultimate,64,9434899,3.556417
512,naive,0,557058558,0.481880
512,optimized,0,105669088,2.540340
512,blocked,32,147348918,1.821767
512,blocked,48,147408844,1.821027
512,blocked,64,183439190,1.463348
512,blocked,96,182888670,1.467753
512,blocked,128,174111424,1.541745
512,blocked_ultimate,64,74898344,3.583997
768,naive,0,1530656653,0.591883
768,optimized,0,357349543,2.535248
768,blocked,32,479525662,1.889304
768,blocked,48,512926613,1.766275
768,blocked,64,542397196,1.670307
768,blocked,96,613639509,1.476387
768,blocked,128,583112422,1.553679
768,blocked_ultimate,64,248575943,3.644639
1024,naive,0,4736583120,0.453382
1024,optimized,0,854573451,2.512930
1024,blocked,32,1191180442,1.802820
1024,blocked,48,1229022280,1.747311
1024,blocked,64,1322480954,1.623830
1024,blocked,96,1458849133,1.472040
1024,blocked,128,1381641902,1.554298
1024,blocked_ultimate,64,614751652,3.493254
#+end_example

    Size    | Naive  | Optimized | Best Blocked | blocked_ultimate
  --------|--------|-----------|--------------|------------------
  256√ó256 | 1.27   | 2.49      | 1.83 (b=32)  | 3.46
  512√ó512 | 0.49   | 2.50      | 1.86 (b=32)  | 3.66
  768√ó768 | 0.60   | 2.56      | 1.92 (b=32)  | 3.60
  1024¬≤   | 0.47   | 2.56      | 1.83 (b=32)  | 3.69


* Strategy 3: SIMD Vectorization

*AVX2*: Process 4 double-precision floats in parallel

#+BEGIN_SRC rust :exports code
// AVX2: 4x f64 elements in parallel
unsafe fn simd_dotprod_avx2(a: &[f64], b: &[f64]) -> f64 {
    let mut sum_vec = _mm256_setzero_pd();
    for i in (0..len).step_by(4) {
        let a_vec = _mm256_loadu_pd(a.as_ptr().add(i));
        let b_vec = _mm256_loadu_pd(b.as_ptr().add(i));
        sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(a_vec, b_vec));
    }
    // Sum horizontal + remainder
}
#+END_SRC

*Benefit*: 4x computational throughput (when not memory-bound)

* Strategy 4: Performance Profiling with perf

Using hardware performance counters to find the *real* bottleneck:

#+BEGIN_SRC sh
./profile.sh simd 512 stat    # Basic performance stats
./profile.sh simd 512 cache   # Detailed cache analysis
./compare_perf.sh 512 cache   # Compare all implementations
#+END_SRC

*Key metrics*:
- L1 dcache load misses (cache locality)
- Instructions per cycle (IPC)
- Memory Bound vs Core Bound (what's limiting us?)
- Retiring percentage (useful work vs stalls)

*Discovery*: SIMD was doing 13.6B instructions but nalgebra only 1.5B!

* Live Demo: Profiling Comparison

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Compare cache behavior of all implementations
./compare_perf.sh 128 cache 2>&1 | grep -E "(Profiling|Time:|L1-dcache-load-misses)" | head -20
#+END_SRC

Watch how L1 miss rate drops:
- Naive: ~48% (cache catastrophe)
- Blocked: ~46% (helps L3, not L1)
- SIMD: ~8% (column-major fixes it)
- Optimized: ~2% (near perfect!)
  

* The Smoking Gun: 49% Overhead

Perf profiling revealed we were spending 49% of execution time on *redundant work*:

#+BEGIN_SRC rust
for i in ii..i_end {
    fill_row_buf(i);           // Load A data: 4,096 elements
    for j in jj..j_end {
        fill_col_buf(j);       // ‚Üê PROBLEM: Loaded 4,096 times!
        dotprod(row_buf, col_buf);
    }
}
#+END_SRC

*The bug*: Column buffer filled once per (i,j) pair instead of once per j
*Result*: 262,144 redundant buffer fills per 64√ó64 block

* The Fix: Reuse Column Buffers

#+BEGIN_SRC rust
// Pre-fill ALL column buffers once
let col_bufs = prefill_all_columns(jj..j_end);  // 64 buffers

for i in ii..i_end {
    fill_row_buf(i);
    for j in jj..j_end {
        let col_buf = &col_bufs[j];  // ‚Üê REUSE!
        dotprod(row_buf, col_buf);
    }
}
#+END_SRC

*Impact*:
- Time: 1.49s ‚Üí 0.65s (*2.3x speedup*)
- Instructions: 13.6B ‚Üí 4.8B (*2.8x fewer*)
- L1 miss rate: 10.1% ‚Üí 2.2% (*4.6x better*)

* What Didn't Work: FMA Instructions

Before profiling, we tried *FMA* (fused multiply-add) - single instruction for multiply+add:

#+BEGIN_SRC rust
// SIMD: separate multiply + add
let prod = _mm256_mul_pd(a_vec, b_vec);
sum_vec = _mm256_add_pd(sum_vec, prod);

// FMA: fused multiply-add (faster, right?)
sum_vec = _mm256_fmadd_pd(a_vec, b_vec, sum_vec);
#+END_SRC

*Result*: 1.49s ‚Üí 1.49s (*0% improvement!*)

*Why?* We weren't bottlenecked by multiply-add operations. The real problem was algorithmic (redundant buffer fills).

*Lesson*: Don't guess! Profile first, then optimize what actually matters.

* Performance Comparison: The Numbers

512√ó512 matrices, 20 iterations:

| Implementation | Time    | L1 Miss Rate | Speedup |
|----------------+---------+--------------+---------|
| Naive          | 11.36s  | 48.5% üí•     | 1.0x    |
| Blocked        | 3.04s   | 45.8%        | 3.7x    |
| SIMD           | 1.49s   | 8.5%         | 7.7x    |
| *Optimized*    | *0.65s* | *2.2%* ‚úÖ    | *17.5x* |
| nalgebra       | 0.21s   | 2.8%         | 54x     |

*Gap to production library*: Only 3x slower!

* Live Benchmark: Full Scaling Analysis

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release -- --scaling 1 | grep -E "(16x16|128x128|512x512|1024x1024)" | head -12
#+END_SRC

* Why GPUs Excel at Matrix Multiplication

*Massive parallelism*: While CPUs have 4-16 cores, GPUs have thousands of cores

#+BEGIN_QUOTE
"Intel Alder Lake: 12 cores, ~3 GHz
NVIDIA RTX 4090: 16,384 CUDA cores, ~2.5 GHz

Matrix multiplication is *embarrassingly parallel* - perfect for GPUs!"
#+END_QUOTE

* ArrayFire: High-Performance GPU Computing in Rust

#+BEGIN_SRC rust :exports code
use arrayfire::*;

// Create matrices on GPU
let a = randu::<f32>(dim4!(1024, 1024, 1, 1));
let b = randu::<f32>(dim4!(1024, 1024, 1, 1));

// GPU matrix multiplication (single line!)
let c = matmul(&a, &b, MatProp::NONE, MatProp::NONE);

// ArrayFire handles:
// - Memory transfer to/from GPU
// - Optimal kernel selection
// - Multi-device management
#+END_SRC

* GPU Performance Potential

Theoretical speedup for 1024√ó1024:

| Implementation | Time (est.) | Speedup vs Naive |
|----------------+-------------+------------------|
| Naive CPU      | ~2400ms     | 1x               |
| Our SIMD       | 463ms       | *5.2x*           |
| GPU (OpenCL)   | ~50-100ms   | *24-48x*         |

*Note*: GPU shines for large matrices (>1024√ó1024), smaller sizes suffer from transfer overhead

* What We Learned

1. *Profile first, optimize second* - FMA gave 0%, profiling found 49% overhead
2. *Cache hierarchy is multi-level* - Blocking helps L3, column-major helps L1
3. *Algorithmic beats instruction-level* - 2.3x from fixing structure vs 0% from FMA
4. *Measure with hardware counters* - L1 miss rates reveal true bottlenecks
5. *Perfect is the enemy of good* - 3x from nalgebra requires complete rewrite

 * The Performance Ladder

512√ó512 matrices (20 iterations):

- Naive: 11,360 ms
- Blocked: 3,040 ms (3.7x faster)
- SIMD: 1,490 ms (7.7x faster)
- *Optimized*: *650 ms* (*17.5x faster*)
- nalgebra: 210 ms (*Gap*: 3x)

*Next 3x would require*: Register tiling, micro-kernels, matrix packing

* Why Are We Still 3x Slower?

nalgebra does **6.8x fewer L1 loads** (1.8B vs 265M):

*Our bottlenecks*:
1. *Buffer copying overhead* - Load matrix ‚Üí buffer ‚Üí dotprod (3x penalty)
2. *Result accumulation* - result.get(i,j) every k-block (2M extra loads)
3. *K-block redundancy* - Each element loaded 8 times (once per k-iteration)

*nalgebra's advantage*:
- Micro-kernels with register blocking
- Keep partial sums in CPU registers (not memory)
- Direct matrix access (no intermediate buffers)
- Single load per matrix element

*To close the gap*: Complete algorithm rewrite with register tiling

* Next Steps

** What Works
- ‚úÖ Cache blocking (3.7x improvement)
- ‚úÖ Column-major layout (reduces L1 misses 5x)
- ‚úÖ SIMD vectorization (parallelism)
- ‚úÖ Profiling-driven optimization (found 49% overhead)

** What Doesn't
- ‚ùå FMA instructions (0% improvement when algorithm bottlenecked)
- ‚ùå More blocking alone (can't fix buffer copy overhead)

** To Match Production Libraries
- Register tiling (keep 4√ó4 sub-blocks in registers)
- Micro-kernels (eliminate intermediate buffers)
- Matrix packing (optimize memory layout once)
- Multi-threading (Rayon for outer loops)

* Questions?

Ready to run any tests you want to see!

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Interactive demo space
echo "Ready for questions and live demos!"
#+END_SRC
