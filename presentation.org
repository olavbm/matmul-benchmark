#+TITLE: High-Performance Matrix Multiplication in Rust
#+AUTHOR: Olav
#+DATE: 2025-10-17
#+STARTUP: overview
#+OPTIONS: toc:nil num:nil

* Introduction

High-Performance Matrix Multiplication in Rust

* Why Matrix Multiplication Matters
Matrix multiplication is *everywhere* in modern computing:
- *Your screen right now*: Graphics transforms
- *DOM rendering*: 2D transform matrices
- *Video playback*: FT, compression/decompression
- *AI/ML inference*: neural network layers
- *Audio processing*: FT, digital signal processing, equalizers, spatial audio

*The bottleneck*: CPUs perform ~10‚Åπ operations/second, but naive matmul is naive

We can do better.
* What is Matrix Multiplication?
** Scalars
5 * 3 = 15

* What is Matrix Multiplication?
** Scalars
** Vectors

         
         
                  
[ 5 3 1 ]
         
         
* What is Matrix Multiplication?
** Scalars
** Vectors

         [ 2
           6
           1 ]        
[ 5 3 1 ] = [ 5 * 2
            + 3 * 6
            + 1 * 1 ]
          = 29
* What is Matrix Multiplication?
** Scalars
** Vectors

         [ 2
           6
           1 ]        
[ 5 3 1 ] = [ 5 * 2
            + 3 * 6
            + 1 * 1 ]
          = 29
          
Also called the dot product.

* What is Matrix Multiplication?
** Scalars
** Vectors
** Matrices
manim!

* What is Matrix Multiplication?
** Lots of mul and add
** Memory Access
** Highly parallelizable 

* What is Matrix Multiplication?
** Lots of mul and add
** Memory Access
Read heavy!
** Highly parallelizable 
Each resulting C[i][j] can be computed separately!

* Starting Point: Naive Implementation

#+BEGIN_SRC rust
// Naive implementation - O(n¬≥)
// Square n*n matrices
for i in 0..n {
    for j in 0..n {
        for k in 0..n {
            c[i][j] += a[i][k] * b[k][j];
        }
    }
}
#+END_SRC

*The problem*: `b[k][j]` causes cache misses - jumping across rows in memory!
* Constraints
** Compute Constraints
- Multiple ALUs (compute units) per core
  - Can do ~2-4 operations per cycle (mul + add)
- Instruction Pipeline
  - Pipeline stalls on dependencies, branches, cache misses

* The Journey from Naive to Optimized

Four key optimization strategies:

1. *Column-major layout* - Fix cache locality
2. *Cache blocking (tiling)* - Fit data in L1 cache
3. *SIMD vectorization* - Process 4 numbers at once
4. *Performance profiling* - Find and fix hidden bottlenecks

*Result*: 17.5x speedup on 512√ó512 matrices (11.36s ‚Üí 0.65s)

* Live Demo: Naive Performance

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release -- --scaling 1
#+END_SRC

#+RESULTS:

* Strategy 1: Column-Major Layout

*Problem*: Row-major matrix B accessed by columns ‚Üí cache disaster

*Solution*: Store matrix in column-major order for efficient column access

#+BEGIN_SRC rust :exports code
// Now accessing B by columns is cache-friendly!
for i in 0..n {
    for j in 0..n {
        for k in 0..n {
            c[i][j] += a[i][k] * b_col_major[j][k];  // Sequential!
        }
    }
}
#+END_SRC

*Impact*: Reduces L1 cache misses from 48% to ~10%

* Strategy 2: Cache Blocking (Tiling)

*Cache blocking* - the 6-nested loop algorithm:
- 3 outer loops iterate over blocks
- 3 inner loops compute within blocks
- Split matrices into 64√ó64 blocks (32KB fits in L1d: 37KB)
- Process blocks instead of full rows/columns
- Reuse data while it's still in cache

*Implementation*: See `blocked_matmul()` in `src/implementations.rs`

* Block Size Analysis

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release -- --blocked 2>&1 | head -20
#+END_SRC

*Hardware constraints*:
- *L1d*: 37 KB per core ‚Üí 64√ó64 blocks optimal
- *L2*: 1.5 MB per core ‚Üí crossover at ~384√ó384
- *L3*: 18 MB shared ‚Üí dramatic speedup at 512√ó512+

* Strategy 3: SIMD Vectorization

*AVX2*: Process 4 double-precision floats in parallel

#+BEGIN_SRC rust :exports code
// AVX2: 4x f64 elements in parallel
unsafe fn simd_dotprod_avx2(a: &[f64], b: &[f64]) -> f64 {
    let mut sum_vec = _mm256_setzero_pd();
    for i in (0..len).step_by(4) {
        let a_vec = _mm256_loadu_pd(a.as_ptr().add(i));
        let b_vec = _mm256_loadu_pd(b.as_ptr().add(i));
        sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(a_vec, b_vec));
    }
    // Sum horizontal + remainder
}
#+END_SRC

*Benefit*: 4x computational throughput (when not memory-bound)

* Strategy 4: Performance Profiling with perf

Using hardware performance counters to find the *real* bottleneck:

#+BEGIN_SRC sh
./profile.sh simd 512 stat    # Basic performance stats
./profile.sh simd 512 cache   # Detailed cache analysis
./compare_perf.sh 512 cache   # Compare all implementations
#+END_SRC

*Key metrics*:
- L1 dcache load misses (cache locality)
- Instructions per cycle (IPC)
- Memory Bound vs Core Bound (what's limiting us?)
- Retiring percentage (useful work vs stalls)

*Discovery*: SIMD was doing 13.6B instructions but nalgebra only 1.5B!

* Live Demo: Profiling Comparison

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Compare cache behavior of all implementations
./compare_perf.sh 128 cache 2>&1 | grep -E "(Profiling|Time:|L1-dcache-load-misses)" | head -20
#+END_SRC

Watch how L1 miss rate drops:
- Naive: ~48% (cache catastrophe)
- Blocked: ~46% (helps L3, not L1)
- SIMD: ~8% (column-major fixes it)
- Optimized: ~2% (near perfect!)

* The Smoking Gun: 49% Overhead

Perf profiling revealed we were spending 49% of execution time on *redundant work*:

#+BEGIN_SRC rust
for i in ii..i_end {
    fill_row_buf(i);           // Load A data: 4,096 elements
    for j in jj..j_end {
        fill_col_buf(j);       // ‚Üê PROBLEM: Loaded 4,096 times!
        dotprod(row_buf, col_buf);
    }
}
#+END_SRC

*The bug*: Column buffer filled once per (i,j) pair instead of once per j
*Result*: 262,144 redundant buffer fills per 64√ó64 block

* The Fix: Reuse Column Buffers

#+BEGIN_SRC rust
// Pre-fill ALL column buffers once
let col_bufs = prefill_all_columns(jj..j_end);  // 64 buffers

for i in ii..i_end {
    fill_row_buf(i);
    for j in jj..j_end {
        let col_buf = &col_bufs[j];  // ‚Üê REUSE!
        dotprod(row_buf, col_buf);
    }
}
#+END_SRC

*Impact*:
- Time: 1.49s ‚Üí 0.65s (*2.3x speedup*)
- Instructions: 13.6B ‚Üí 4.8B (*2.8x fewer*)
- L1 miss rate: 10.1% ‚Üí 2.2% (*4.6x better*)

* What Didn't Work: FMA Instructions

Before profiling, we tried *FMA* (fused multiply-add) - single instruction for multiply+add:

#+BEGIN_SRC rust
// SIMD: separate multiply + add
let prod = _mm256_mul_pd(a_vec, b_vec);
sum_vec = _mm256_add_pd(sum_vec, prod);

// FMA: fused multiply-add (faster, right?)
sum_vec = _mm256_fmadd_pd(a_vec, b_vec, sum_vec);
#+END_SRC

*Result*: 1.49s ‚Üí 1.49s (*0% improvement!*)

*Why?* We weren't bottlenecked by multiply-add operations. The real problem was algorithmic (redundant buffer fills).

*Lesson*: Don't guess! Profile first, then optimize what actually matters.

* Performance Comparison: The Numbers

512√ó512 matrices, 20 iterations:

| Implementation | Time    | L1 Miss Rate | Speedup |
|----------------+---------+--------------+---------|
| Naive          | 11.36s  | 48.5% üí•     | 1.0x    |
| Blocked        | 3.04s   | 45.8%        | 3.7x    |
| SIMD           | 1.49s   | 8.5%         | 7.7x    |
| *Optimized*    | *0.65s* | *2.2%* ‚úÖ    | *17.5x* |
| nalgebra       | 0.21s   | 2.8%         | 54x     |

*Gap to production library*: Only 3x slower!

* Live Benchmark: Full Scaling Analysis

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release -- --scaling 1 | grep -E "(16x16|128x128|512x512|1024x1024)" | head -12
#+END_SRC

* Hardware Constraints

- *L1d*: 37 KB per core ‚Üí 64√ó64 blocks optimal
- *L2*: 1.5 MB per core ‚Üí performance crossover at ~384√ó384
- *L3*: 18 MB shared ‚Üí dramatic speedup at 512√ó512+

* Performance Visualization

#+BEGIN_SRC sh :results file :file performance.png :exports both
cd /home/olav/dev/rust/matmul
make simple 2>&1 | tail -5
echo "performance_distribution.png"
#+END_SRC

* Try Different Matrix Sizes

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Change the size here and re-run!
SIZE=256
cargo run --release -- --scaling 1 | grep "${SIZE}x${SIZE}" | head -3
#+END_SRC

* Compare Specific Algorithms

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Benchmarks organized into categories: mm (matrix mult), simd, vector_dotprod
cargo +nightly bench simd 2>&1 | grep "test benches::" | head -5
#+END_SRC

*Organized benchmarks*: Easy to run just what you need!
- `cargo bench vector_dotprod` - Pure dot product comparison
- `cargo bench mm` - Matrix multiplication algorithms
- `cargo bench simd` - SIMD-accelerated variants

* Why GPUs Excel at Matrix Multiplication

*Massive parallelism*: While CPUs have 4-16 cores, GPUs have thousands of cores

#+BEGIN_QUOTE
"Intel Alder Lake: 12 cores, ~3 GHz
NVIDIA RTX 4090: 16,384 CUDA cores, ~2.5 GHz

Matrix multiplication is *embarrassingly parallel* - perfect for GPUs!"
#+END_QUOTE

* ArrayFire: High-Performance GPU Computing in Rust

#+BEGIN_SRC rust :exports code
use arrayfire::*;

// Create matrices on GPU
let a = randu::<f32>(dim4!(1024, 1024, 1, 1));
let b = randu::<f32>(dim4!(1024, 1024, 1, 1));

// GPU matrix multiplication (single line!)
let c = matmul(&a, &b, MatProp::NONE, MatProp::NONE);

// ArrayFire handles:
// - Memory transfer to/from GPU
// - Optimal kernel selection
// - Multi-device management
#+END_SRC

* GPU Performance Potential

Theoretical speedup for 1024√ó1024:

| Implementation | Time (est.) | Speedup vs Naive |
|----------------+-------------+------------------|
| Naive CPU      | ~2400ms     | 1x               |
| Our SIMD       | 463ms       | *5.2x*           |
| GPU (OpenCL)   | ~50-100ms   | *24-48x*         |
| GPU (CUDA)     | ~10-20ms    | *120-240x*       |

*Note*: GPU shines for large matrices (>1024√ó1024), smaller sizes suffer from transfer overhead

* Live Demo: GPU Setup

#+BEGIN_SRC sh :results output :exports both
# Check if we have GPU compute available
if command -v clinfo >/dev/null 2>&1; then
    echo "OpenCL Status:"
    clinfo -l 2>/dev/null || echo "OpenCL runtime needs installation"
else
    echo "Install OpenCL: sudo apt install clinfo ocl-icd-opencl-dev intel-opencl-icd"
fi

# Check ArrayFire availability
if cargo tree 2>/dev/null | grep -q arrayfire; then
    echo -e "\nArrayFire: Installed ‚úì"
else
    echo -e "\nTo add GPU support: cargo add arrayfire"
fi
#+END_SRC

* What We Learned

1. *Profile first, optimize second* - FMA gave 0%, profiling found 49% overhead
2. *Cache hierarchy is multi-level* - Blocking helps L3, column-major helps L1
3. *Algorithmic beats instruction-level* - 2.3x from fixing structure vs 0% from FMA
4. *Measure with hardware counters* - L1 miss rates reveal true bottlenecks
5. *Perfect is the enemy of good* - 3x from nalgebra requires complete rewrite

 * The Performance Ladder

512√ó512 matrices (20 iterations):

- Naive: 11,360 ms
- Blocked: 3,040 ms (3.7x faster)
- SIMD: 1,490 ms (7.7x faster)
- *Optimized*: *650 ms* (*17.5x faster*)
- nalgebra: 210 ms (*Gap*: 3x)

*Next 3x would require*: Register tiling, micro-kernels, matrix packing

* Why Are We Still 3x Slower?

nalgebra does **6.8x fewer L1 loads** (1.8B vs 265M):

*Our bottlenecks*:
1. *Buffer copying overhead* - Load matrix ‚Üí buffer ‚Üí dotprod (3x penalty)
2. *Result accumulation* - result.get(i,j) every k-block (2M extra loads)
3. *K-block redundancy* - Each element loaded 8 times (once per k-iteration)

*nalgebra's advantage*:
- Micro-kernels with register blocking
- Keep partial sums in CPU registers (not memory)
- Direct matrix access (no intermediate buffers)
- Single load per matrix element

*To close the gap*: Complete algorithm rewrite with register tiling

* Next Steps

** What Works
- ‚úÖ Cache blocking (3.7x improvement)
- ‚úÖ Column-major layout (reduces L1 misses 5x)
- ‚úÖ SIMD vectorization (parallelism)
- ‚úÖ Profiling-driven optimization (found 49% overhead)

** What Doesn't
- ‚ùå FMA instructions (0% improvement when algorithm bottlenecked)
- ‚ùå More blocking alone (can't fix buffer copy overhead)

** To Match Production Libraries
- Register tiling (keep 4√ó4 sub-blocks in registers)
- Micro-kernels (eliminate intermediate buffers)
- Matrix packing (optimize memory layout once)
- Multi-threading (Rayon for outer loops)

* Questions?

Ready to run any tests you want to see!

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Interactive demo space
echo "Ready for questions and live demos!"
#+END_SRC
