#+TITLE: High-Performance Matrix Multiplication in Rust
#+AUTHOR: Olav
#+DATE: 2025-10-17
#+STARTUP: overview
#+OPTIONS: toc:nil num:nil

* Introduction

High-Performance Matrix Multiplication in Rust

* Introduction

+High-Performance Matrix Multiplication in Rust+
How CPUs work - a Rust perspective

* Why Matrix Multiplication Matters

Matrix multiplication is *everywhere** in modern computing:

- *Your screen right now*: Graphics transforms
- *Scrolling*: 2D transform matrices
- *Video playback*: FT, compression/decompression
- *AI/ML inference*: neural network layers
- *Physics simulations*: Rigid body dynamics, finite element analysis
- *Audio processing*: FT, Digital signal processing, equalizers, spatial audio

*The bottleneck*: CPUs perform ~10⁹ operations/second, but naive matmul is naive

We can do better.

* The Challenge

#+BEGIN_QUOTE
"A 1024×1024 matrix multiplication requires 2.1 billion floating point operations.

Your CPU can theoretically do this in ~2 seconds.

A naive implementation takes *minutes*. Why?"
#+END_QUOTE

* The Journey from Naive to SIMD

- Started with basic O(n³) implementation (simple but slow)
- Applied cache optimization techniques (64×64 blocking for L1d cache)
- Integrated SIMD vectorization (AVX2 - 4x f64 parallelism)
- Refactored for clarity with macros and documentation
- Achieved competitive performance with hand-tuned libraries

*Result*: 5.2x speedup on 1024×1024 matrices
*Bonus*: We beat nalgebra's dot product! (271ns vs 288ns)

* Cache Misses Kill Performance

#+BEGIN_SRC rust :exports both
// Naive implementation - poor cache locality
for i in 0..n {
    for j in 0..n {
        for k in 0..n {
            c[i][j] += a[i][k] * b[k][j];  // b[k][j] = cache miss!
        }
    }
}
#+END_SRC

* Live Demo: Naive Performance

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release -- --scaling 1
#+END_SRC

#+RESULTS:

* Divide and Conquer for Cache

*Cache blocking* - the 6-nested loop algorithm:
- 3 outer loops iterate over blocks
- 3 inner loops compute within blocks
- Split matrices into 64×64 blocks (32KB fits in L1d: 37KB)
- Process blocks instead of full rows/columns
- Reuse data while it's still in cache

*Implementation*: See `blocked_matmul()` in `src/implementations.rs`
Now with proper documentation and configurable block size constant!

* Block Size Analysis

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release -- --blocked 2>&1 | head -20
#+END_SRC

* Process 4 Numbers at Once

#+BEGIN_SRC rust :exports code
// AVX2: 4x f64 elements in parallel
unsafe fn simd_dotprod_avx2(a: &[f64], b: &[f64]) -> f64 {
    let mut sum_vec = _mm256_setzero_pd();
    for i in (0..len).step_by(4) {
        let a_vec = _mm256_loadu_pd(a.as_ptr().add(i));
        let b_vec = _mm256_loadu_pd(b.as_ptr().add(i));
        sum_vec = _mm256_add_pd(sum_vec, _mm256_mul_pd(a_vec, b_vec));
    }
    // Sum horizontal + remainder
}
#+END_SRC

* Dot Product Benchmark: We Beat nalgebra!

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Run just the vector dotprod benchmarks (organized with macros!)
cargo +nightly bench vector_dotprod 2>&1 | grep "test benches::" | tail -8
#+END_SRC

*Clean benchmark organization*: 60+ benchmarks now generated from 5 macros!

* Performance Comparison: The Numbers

| Matrix Size | Naive    | Blocked  | SIMD     | Speedup |
|-------------+----------+----------+----------+---------|
| 256×256     | ~30ms    | ~13.4ms  | *8.9ms*  | 3.4x    |
| 512×512     | ~300ms   | ~120ms   | *76.8ms* | 3.9x    |
| 1024×1024   | ~2400ms  | ~800ms   | *463ms*  | 5.2x    |

* Live Benchmark: Full Scaling Analysis

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
cargo run --release -- --scaling 1 | grep -E "(16x16|128x128|512x512|1024x1024)" | head -12
#+END_SRC

* Hardware Constraints

- *L1d*: 37 KB per core → 64×64 blocks optimal
- *L2*: 1.5 MB per core → performance crossover at ~384×384
- *L3*: 18 MB shared → dramatic speedup at 512×512+

* Performance Visualization

#+BEGIN_SRC sh :results file :file performance.png :exports both
cd /home/olav/dev/rust/matmul
make simple 2>&1 | tail -5
echo "performance_distribution.png"
#+END_SRC

* Try Different Matrix Sizes

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Change the size here and re-run!
SIZE=256
cargo run --release -- --scaling 1 | grep "${SIZE}x${SIZE}" | head -3
#+END_SRC

* Compare Specific Algorithms

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Benchmarks organized into categories: mm (matrix mult), simd, vector_dotprod
cargo +nightly bench simd 2>&1 | grep "test benches::" | head -5
#+END_SRC

*Organized benchmarks*: Easy to run just what you need!
- `cargo bench vector_dotprod` - Pure dot product comparison
- `cargo bench mm` - Matrix multiplication algorithms
- `cargo bench simd` - SIMD-accelerated variants

* Why GPUs Excel at Matrix Multiplication

*Massive parallelism*: While CPUs have 4-16 cores, GPUs have thousands of cores

#+BEGIN_QUOTE
"Intel Alder Lake: 12 cores, ~3 GHz
NVIDIA RTX 4090: 16,384 CUDA cores, ~2.5 GHz

Matrix multiplication is *embarrassingly parallel* - perfect for GPUs!"
#+END_QUOTE

* ArrayFire: High-Performance GPU Computing in Rust

#+BEGIN_SRC rust :exports code
use arrayfire::*;

// Create matrices on GPU
let a = randu::<f32>(dim4!(1024, 1024, 1, 1));
let b = randu::<f32>(dim4!(1024, 1024, 1, 1));

// GPU matrix multiplication (single line!)
let c = matmul(&a, &b, MatProp::NONE, MatProp::NONE);

// ArrayFire handles:
// - Memory transfer to/from GPU
// - Optimal kernel selection
// - Multi-device management
#+END_SRC

* GPU Performance Potential

Theoretical speedup for 1024×1024:

| Implementation | Time (est.) | Speedup vs Naive |
|----------------+-------------+------------------|
| Naive CPU      | ~2400ms     | 1x               |
| Our SIMD       | 463ms       | *5.2x*           |
| GPU (OpenCL)   | ~50-100ms   | *24-48x*         |
| GPU (CUDA)     | ~10-20ms    | *120-240x*       |

*Note*: GPU shines for large matrices (>1024×1024), smaller sizes suffer from transfer overhead

* Live Demo: GPU Setup

#+BEGIN_SRC sh :results output :exports both
# Check if we have GPU compute available
if command -v clinfo >/dev/null 2>&1; then
    echo "OpenCL Status:"
    clinfo -l 2>/dev/null || echo "OpenCL runtime needs installation"
else
    echo "Install OpenCL: sudo apt install clinfo ocl-icd-opencl-dev intel-opencl-icd"
fi

# Check ArrayFire availability
if cargo tree 2>/dev/null | grep -q arrayfire; then
    echo -e "\nArrayFire: Installed ✓"
else
    echo -e "\nTo add GPU support: cargo add arrayfire"
fi
#+END_SRC

* What We Learned

1. *Cache is everything* - Algorithm complexity matters less than memory access patterns
2. *Blocking is powerful* - Simple technique, massive gains (5.2x speedup)
3. *SIMD closes the gap* - We're now only 7x slower than BLAS (was 10x)
4. *Measure everything* - Statistical analysis reveals the truth
5. *GPUs change the game* - Massive parallelism unlocks 100x+ speedups for large matrices

* The Performance Ladder

- Our SIMD: 1,734 ms
- Intel MKL BLAS: 247 ms (*Gap*: 7x)
- GPU (projected): ~20-50 ms (*Gap*: 35-87x faster than SIMD!)

* Next Steps: Performance

** Low-Hanging Fruit
- FMA instructions (fused multiply-add) for better throughput
- Memory prefetching in SIMD loops
- Cache line alignment (64-byte boundaries)

** Advanced Optimizations
- Multi-level cache blocking (L1 + L2 + L3 hierarchical)
- Thread-level parallelism (Rayon?)
- Mixed precision (f16/f32/f64)
- Specialized rectangular matrix kernels

** Other Improvements
- Property-based testing (quickcheck)
- CI/CD pipeline with performance regression testing

* Questions?

Ready to run any tests you want to see!

#+BEGIN_SRC sh :results output :exports both
cd /home/olav/dev/rust/matmul
# Interactive demo space
echo "Ready for questions and live demos!"
#+END_SRC
